{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Threshold Value: 4.4408920985006264e-17\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator HistGradientBoostingClassifier should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Feature selection with SelectFromModel using the quantile threshold\u001b[39;00m\n\u001b[0;32m     42\u001b[0m selector \u001b[38;5;241m=\u001b[39m SelectFromModel(hgb, threshold\u001b[38;5;241m=\u001b[39mthreshold_value, prefit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 43\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mfit_transform(X_train_scaled, y_train)\n\u001b[0;32m     44\u001b[0m X_test_selected \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform(X_test_scaled)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Hyperparameter tuning using GridSearchCV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:115\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    108\u001b[0m     X,\n\u001b[0;32m    109\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    114\u001b[0m )\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:119\u001b[0m, in \u001b[0;36mSelectorMixin._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce X to the selected features.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_support()\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    121\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    122\u001b[0m             (\n\u001b[0;32m    123\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features were selected: either the data is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:72\u001b[0m, in \u001b[0;36mSelectorMixin.get_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_support\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Get a mask, or integer index, of the features selected.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        values are indices into the input feature vector.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_support_mask()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indices \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:299\u001b[0m, in \u001b[0;36mSelectFromModel._get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(max_features, Integral):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_features` must be an integer. Got `max_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m scores \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    300\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    301\u001b[0m     getter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    302\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    303\u001b[0m     norm_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_order,\n\u001b[0;32m    304\u001b[0m )\n\u001b[0;32m    305\u001b[0m threshold \u001b[38;5;241m=\u001b[39m _calculate_threshold(estimator, scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:233\u001b[0m, in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, getter, transform_func, norm_order)\u001b[0m\n\u001b[0;32m    231\u001b[0m         getter \u001b[38;5;241m=\u001b[39m attrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `importance_getter==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, the underlying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling transform.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     getter \u001b[38;5;241m=\u001b[39m attrgetter(getter)\n",
      "\u001b[1;31mValueError\u001b[0m: when `importance_getter=='auto'`, the underlying estimator HistGradientBoostingClassifier should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('new_descriptors.csv')\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the HistGradientBoosting model\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "hgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Feature importance using permutation_importance\n",
    "perm_importance = permutation_importance(hgb, X_train_scaled, y_train, n_repeats=10, random_state=42)\n",
    "importances = perm_importance.importances_mean\n",
    "\n",
    "# Determine the threshold as a quantile (e.g., 75th percentile)\n",
    "quantile = 0.75\n",
    "threshold_value = np.quantile(importances, quantile)\n",
    "print(f\"Quantile Threshold Value: {threshold_value}\")\n",
    "\n",
    "# Feature selection with SelectFromModel using the quantile threshold\n",
    "selector = SelectFromModel(hgb, threshold=threshold_value, prefit=False)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_leaf': [10, 20, 30]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=HistGradientBoostingClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "best_hgb = grid_search.best_estimator_\n",
    "\n",
    "# Train HistGradientBoosting with the best model\n",
    "hgb_final = HistGradientBoostingClassifier(learning_rate=grid_search.best_params_['learning_rate'],\n",
    "                                           max_iter=grid_search.best_params_['max_iter'],\n",
    "                                           max_depth=grid_search.best_params_['max_depth'],\n",
    "                                           min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
    "                                           random_state=42)\n",
    "hgb_final.fit(X_train_selected, y_train)\n",
    "y_pred_selected = hgb_final.predict(X_test_selected)\n",
    "\n",
    "# Cross-validation scores for HistGradientBoosting\n",
    "cv_scores_selected = cross_val_score(hgb_final, X_train_selected, y_train, cv=5)\n",
    "\n",
    "# Print results\n",
    "print(\"HistGradientBoosting with Feature Importance Selection\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_selected))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_selected))\n",
    "print(\"Cross-Validation Scores (Feature Importance):\", cv_scores_selected)\n",
    "print(\"Mean CV Score (Feature Importance):\", np.mean(cv_scores_selected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Threshold Value: 4.4408920985006264e-17\n",
      "Selected Features:\n",
      "Index(['nAcid', 'SpMax_A', 'SpMAD_A', 'VE1_A', 'VR2_A', 'ATS0dv', 'ATS8dv',\n",
      "       'ATS4s', 'ATS8s', 'ATS0m',\n",
      "       ...\n",
      "       'GGI3', 'GGI8', 'JGI2', 'JGI6', 'JGI7', 'JGI8', 'MWC08', 'SRW08',\n",
      "       'SRW09', 'SRW10'],\n",
      "      dtype='object', length=370)\n",
      "Cross-Validation Accuracy Scores: [0.81545064 0.83118741 0.90701001 0.9241774  0.92550143]\n",
      "Mean Cross-Validation Accuracy: 0.8806653795229369\n",
      "Rows predicted as inhibitors:\n",
      "      nAcid  nBase    SpAbs_A   SpMax_A  SpDiam_A     SpAD_A   SpMAD_A  \\\n",
      "325       0      3  42.760646  2.508523  5.015985  42.760646  1.257666   \n",
      "2480      0      0  36.286579  2.462812  4.918740  36.286579  1.251261   \n",
      "1862      0      0  39.235488  2.421269  4.816843  39.235488  1.307850   \n",
      "2374      0      2  37.725101  2.444531  4.883653  37.725101  1.300866   \n",
      "3356      0      0  38.839018  2.499519  4.851194  38.839018  1.294634   \n",
      "...     ...    ...        ...       ...       ...        ...       ...   \n",
      "1647      1      0  41.585356  2.511985  5.023970  41.585356  1.223099   \n",
      "1949      0      0  43.175258  2.509731  4.936039  43.175258  1.308341   \n",
      "2271      0      0  41.999240  2.633193  5.244116  41.999240  1.235272   \n",
      "2965      0      0  36.921710  2.578320  5.156639  36.921710  1.273162   \n",
      "944       0      1  42.881081  2.595466  5.097480  42.881081  1.261208   \n",
      "\n",
      "       LogEE_A     VE1_A     VE2_A  ...     TSRW10          MW       AMW  \\\n",
      "325   4.460046  4.618782  0.135847  ...  84.285631  461.222703  7.439076   \n",
      "2480  4.287808  4.481934  0.154549  ...  72.441094  426.122498  8.522450   \n",
      "1862  4.333920  4.462259  0.148742  ...  79.192596  403.180838  7.753478   \n",
      "2374  4.303736  4.790211  0.165180  ...  78.192972  394.236876  6.681981   \n",
      "3356  4.364205  4.700046  0.156668  ...  83.333464  438.144965  8.591078   \n",
      "...        ...       ...       ...  ...        ...         ...       ...   \n",
      "1647  4.438644  4.674039  0.137472  ...  70.415546  511.083191  9.292422   \n",
      "1949  4.447059  4.787228  0.145068  ...  85.410773  450.186732  7.898013   \n",
      "2271  4.489929  4.942009  0.145353  ...  91.438888  523.064348  9.869139   \n",
      "2965  4.289065  4.285544  0.147777  ...  65.009126  437.106372  8.247290   \n",
      "944   4.481654  4.280184  0.125888  ...  90.719892  513.206420  7.331520   \n",
      "\n",
      "      WPath  WPol  Zagreb1  Zagreb2   mZagreb1  mZagreb2  Target  \n",
      "325    3532    59      184      220  12.645833  7.319444       1  \n",
      "2480   2165    50      152      177  10.541667  6.222222       1  \n",
      "1862   2885    46      158      185   9.111111  6.611111       1  \n",
      "2374   2371    46      154      182   8.861111  6.388889       1  \n",
      "3356   2686    45      170      202   8.923611  6.180556       1  \n",
      "...     ...   ...      ...      ...        ...       ...     ...  \n",
      "1647   3218    57      176      206  13.534722  7.388889       0  \n",
      "1949   3310    56      182      220  10.194444  7.138889       1  \n",
      "2271   3087    70      194      243  14.138889  7.055556       1  \n",
      "2965   2066    54      152      184  10.541667  6.534722       0  \n",
      "944    3539    61      188      228  12.826389  7.250000       1  \n",
      "\n",
      "[802 rows x 1452 columns]\n",
      "\n",
      "HistGradientBoosting with Permutation Importance Selection and SMOTE\n",
      "Overall Accuracy: 0.8039585296889726\n",
      "Precision: 0.8001261479723666\n",
      "F1 Score: 0.8017792559429026\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.59      0.61       277\n",
      "           1       0.86      0.88      0.87       784\n",
      "\n",
      "    accuracy                           0.80      1061\n",
      "   macro avg       0.75      0.74      0.74      1061\n",
      "weighted avg       0.80      0.80      0.80      1061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # Needed to enable HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('new_descriptors.csv')\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the HistGradientBoosting model\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "hgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute permutation feature importance\n",
    "result = permutation_importance(hgb, X_train_scaled, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Determine the threshold as a quantile (e.g., 75th percentile)\n",
    "quantile = 0.75\n",
    "threshold_value = np.quantile(importances, quantile)\n",
    "print(f\"Quantile Threshold Value: {threshold_value}\")\n",
    "\n",
    "# Feature selection based on permutation importance\n",
    "selected_features_mask = importances >= threshold_value\n",
    "selected_features = X.columns[selected_features_mask]\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "X_train_selected = X_train_scaled[:, selected_features_mask]\n",
    "X_test_selected = X_test_scaled[:, selected_features_mask]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [None, 3, 5, 10],\n",
    "    'min_samples_leaf': [20, 50, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=HistGradientBoostingClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "best_hgb = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate model with cross-validation\n",
    "cross_val_scores = cross_val_score(best_hgb, X_train_smote, y_train_smote, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores: {cross_val_scores}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {cross_val_scores.mean()}\")\n",
    "\n",
    "# Train the final model\n",
    "best_hgb.fit(X_train_smote, y_train_smote)\n",
    "y_pred = best_hgb.predict(X_test_selected)\n",
    "\n",
    "# Identify rows predicted as inhibitors\n",
    "# Assume inhibitors are labeled as 1; adjust if necessary\n",
    "inhibitor_label = 1\n",
    "inhibitor_predictions = X_test_selected[y_pred == inhibitor_label]\n",
    "\n",
    "# Map predictions back to original dataframe\n",
    "inhibitor_indices = np.where(y_pred == inhibitor_label)[0]\n",
    "inhibitor_rows = df.iloc[X_test.index[inhibitor_indices]]\n",
    "\n",
    "# Print identified rows\n",
    "print(\"Rows predicted as inhibitors:\")\n",
    "print(inhibitor_rows)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nHistGradientBoosting with Permutation Importance Selection and SMOTE\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Overall Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert selected features to a DataFrame and save as CSV\n",
    "selected_features_df = pd.DataFrame(selected_features, columns=['Selected_Features'])\n",
    "selected_features_df.to_csv('selected_features_hg.csv', index=False)\n",
    "\n",
    "# Save rows predicted as inhibitors to a CSV\n",
    "inhibitor_rows.to_csv('rows_predicted_as_inhibitors_hg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator HistGradientBoostingClassifier should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m selector\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get the selected features\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get the names of the selected features\u001b[39;00m\n\u001b[0;32m     29\u001b[0m selected_feature_names \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns[selector\u001b[38;5;241m.\u001b[39mget_support()]\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:115\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# public Mixin.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    108\u001b[0m     X,\n\u001b[0;32m    109\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    114\u001b[0m )\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:119\u001b[0m, in \u001b[0;36mSelectorMixin._transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce X to the selected features.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_support()\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    121\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    122\u001b[0m             (\n\u001b[0;32m    123\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo features were selected: either the data is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    127\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:72\u001b[0m, in \u001b[0;36mSelectorMixin.get_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_support\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Get a mask, or integer index, of the features selected.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        values are indices into the input feature vector.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_support_mask()\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indices \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:299\u001b[0m, in \u001b[0;36mSelectFromModel._get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(max_features, Integral):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_features` must be an integer. Got `max_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m scores \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    300\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    301\u001b[0m     getter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    302\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    303\u001b[0m     norm_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_order,\n\u001b[0;32m    304\u001b[0m )\n\u001b[0;32m    305\u001b[0m threshold \u001b[38;5;241m=\u001b[39m _calculate_threshold(estimator, scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bdeva\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:233\u001b[0m, in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, getter, transform_func, norm_order)\u001b[0m\n\u001b[0;32m    231\u001b[0m         getter \u001b[38;5;241m=\u001b[39m attrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `importance_getter==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, the underlying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling transform.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     getter \u001b[38;5;241m=\u001b[39m attrgetter(getter)\n",
      "\u001b[1;31mValueError\u001b[0m: when `importance_getter=='auto'`, the underlying estimator HistGradientBoostingClassifier should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset (ensure your CSV has the target column labeled, e.g., 'target')\n",
    "df = pd.read_csv('new_descriptors.csv')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['Target'])  # Replace 'target' with your actual target column name\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (optional, for testing the performance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# Use SelectFromModel with the feature_importances_ after fitting the model\n",
    "selector = SelectFromModel(hgb, prefit=True, threshold=\"mean\")\n",
    "\n",
    "# Transform the training and test sets based on selected features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = X_train.columns[selector.get_support()]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# Optionally: Train a new model on selected features\n",
    "hgb_selected = HistGradientBoostingClassifier(random_state=42)\n",
    "hgb_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = hgb_selected.score(X_test_selected, y_test)\n",
    "print(f\"Model accuracy with selected features: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
